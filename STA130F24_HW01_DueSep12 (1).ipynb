{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaad910f",
   "metadata": {},
   "source": [
    "# STA130 Week 01 Homework \n",
    "\n",
    "Please see the course [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) for the list of topics covered in this homework assignment, and a list of topics that might appear during ChatBot conversations which are \"out of scope\" for the purposes of this homework assignment (and hence can be safely ignored if encountered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aede3d7",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Introduction</u></summary>\n",
    "\n",
    "### Introduction\n",
    "    \n",
    "A reasonable characterization of STA130 Homework is that it simply defines a weekly reading comprehension assignment. \n",
    "Indeed, STA130 Homework essentially boils down to completing various understanding confirmation exercises oriented around coding and writing tasks.\n",
    "However, rather than reading a textbook, STA130 Homework is based on ChatBots so students can interactively follow up to clarify questions or confusion that they may still have regarding learning objective assignments.\n",
    "\n",
    "> Communication is a fundamental skill underlying statistics and data science, so STA130 Homework based on ChatBots helps practice effective two-way communication as part of a \"realistic\" dialogue activity supporting underlying conceptual understanding building. \n",
    "\n",
    "It will likely become increasingly tempting to rely on ChatBots to \"do the work for you\". But when you find yourself frustrated with a ChatBots inability to give you the results you're looking for, this is a \"hint\" that you've become overreliant on the ChatBots. Your objective should not be to have ChatBots \"do the work for you\", but to use ChatBots to help you build your understanding so you can efficiently leverage ChatBots (and other resources) to help you work more efficiently.<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Instructions</u></summary>\n",
    "\n",
    "### Instructions\n",
    "    \n",
    "1. Code and write all your answers (for both the \"Prelecture\" and \"Postlecture\" HW) in a python notebook (in code and markdown cells) \n",
    "    \n",
    "> It is *suggested but not mandatory* that you complete the \"Prelecture\" HW prior to the Monday LEC since (a) all HW is due at the same time; but, (b) completing some of the HW early will mean better readiness for LEC and less of a \"procrastentation cruch\" towards the end of the week...\n",
    "    \n",
    "2. Paste summaries of your ChatBot sessions (including link(s) to chat log histories if you're using ChatGPT) within your notebook\n",
    "    \n",
    "> Create summaries of your ChatBot sessions by using concluding prompts such as \"Please provide a summary of our exchanges here so I can submit them as a record of our interactions as part of a homework assignment\" or, \"Please provide me with the final working verson of the code that we created together\"\n",
    "    \n",
    "3. Save your python jupyter notebook in your own account and \"repo\" on [github.com](github.com) and submit a link to that notebook though Quercus for assignment marking<br><br>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Prompt Engineering?</u></summary>\n",
    "\n",
    "### Prompt Engineering?    \n",
    "    \n",
    "The questions (as copy-pasted prompts) are designed to initialize appropriate ChatBot conversations which can be explored in the manner of an interactive and dynamic textbook; but, it is nonetheless **strongly recommendated** that your rephrase the questions in a way that you find natural to ensure a clear understanding of the question. Given sensible prompts the represent a question well, the two primary challenges observed to arise from ChatBots are \n",
    "\n",
    "1. conversations going beyond the intended scope of the material addressed by the question; and, \n",
    "2. unrecoverable confusion as a result of sequential layers logial inquiry that cannot be resolved. \n",
    "\n",
    "In the case of the former (1), adding constraints specifying the limits of considerations of interest tends to be helpful; whereas, the latter (2) is often the result of initial prompting that leads to poor developments in navigating the material, which are likely just best resolve by a \"hard reset\" with a new initial approach to prompting.  Indeed, this is exactly the behavior [hardcoded into copilot](https://answers.microsoft.com/en-us/bing/forum/all/is-this-even-normal/0b6dcab3-7d6c-4373-8efe-d74158af3c00)...\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77293b06",
   "metadata": {},
   "source": [
    "\n",
    "### Marking Rubric (which may award partial credit) \n",
    "\n",
    "- [0.1 points]: All relevant ChatBot summaries [including link(s) to chat log histories if you're using ChatGPT] are reported within the notebook\n",
    "- [0.2 points]: Reasonable well-written general definitions for Question \"2.2\"\n",
    "- [0.3 points]: Demonstrated understanding regarding Question \"4\"\n",
    "<!-- - [0.2 points]: A sensible justification for the choice in Question \"7.4\" -->\n",
    "- [0.4 points]: Requested assessment of ChatBot versus google performance in Question \"8.3\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8cc949",
   "metadata": {},
   "source": [
    "## \"Prelecture\" HW [*completion prior to next LEC is suggested but not mandatory*]\n",
    "\n",
    "#### 1. Pick one of the datasets from the ChatBot session(s) of the **TUT demo** (or from your own ChatBot session if you wish) and use the code produced through the ChatBot interactions to import the data and confirm that the dataset has missing values<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> If your TA has not shared a relevant ChatBot session from their **TUT demo** through a piazza post and a Quercus announcement, the **TUT notebook** has links to example ChatBot sessions that you can use; or, ...\n",
    "> \n",
    "> ```python\n",
    "> # feel free to just use the following if you prefer...\n",
    "> import pandas as pd\n",
    "> url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "> df = pd.read_csv(url)\n",
    "> df.isna().sum()\n",
    "> ```\n",
    "    \n",
    "</details>\n",
    "\n",
    "#### 2. Start a new ChatBot session with an initial prompt introducing the dataset you're using and request help to determine how many columns and rows of data a `pandas` DataFrame has, and then\n",
    "\n",
    "1. use code provided in your ChatBot session to print out the number of rows and columns of the dataset; and,  \n",
    "2. write your own general definitions of the meaning of \"observations\" and \"variables\" based on asking the ChatBot to explain these terms in the context of your dataset<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> A good initial prompt to start would be be something like\n",
    "> - \"I've downloaded a dataset about characters from animal crossings (from https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv), and I'd like to know what columns of information I have and how much data I have\"\n",
    "> \n",
    "> You can further reduce the scope of your inquiry with if needed with something like\n",
    "> - \"I've already downloaded the data and want to understand the size (or dimensions) of the dataset to start with\"\n",
    "> \n",
    "> *Some ChatBots can upload your data and do this for you; but, extended usage of this feature [likely requires a paid subscription](https://github.com/pointOfive/stat130chat130/blob/main/CHATLOG/wk1/GPT/SLS/00006_gpt3p5_LoadDataPaywall.md); and, anyway, you need to run the code yourself rather than having a ChatBot do that for you; and, for STA130 we don't want a ChatBot to just do the analysis for us; rather, we instead want ChatBots to help us understand the steps we need to take to analyze the data; so,* **you DO NOT need to purchase an upgraded version of any ChatBots**\n",
    "> - Free-tier level ChatBots like [GPT4o-mini](https://chat.openai.com/) or [Copilot](https://copilot.microsoft.com/) (which is partially based on [ChatGPT4.0](https://chat.openai.com/), and which you have access to through your UofT account) are sufficiently sophisticated and perfectly appropriate for the STA130 course\n",
    "    \n",
    "</details>\n",
    "\n",
    "#### 3. Ask the ChatBot how you can provide simple summaries of the columns in the dataset and use the suggested code to provide these summaries for your dataset<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> Use your ChatBot session to help you create working examples of using  `df.describe()` and `df['column'].value_counts()` for your dataset (although note that the `.value_counts()` method is not really meant to be used for numeric variables, so if you dataset has only numeric variables, `.value_counts()` might not be particularly informative...)\n",
    ">\n",
    "> #### ChatBot Response Scope \n",
    ">     \n",
    "> If prompts are not sufficiently focused you will likely get overly broad responses from the ChatBot, but you can always respond with subsequent refinement requests to appropriately limit the scope of the ChatBot responses to focus on addressing your actual content targets; so, \n",
    "> - an initially very general inquiry like, \"I need help analyzing my data\" will likely result in a ChatBot response suggesting a wide variety of approaches and techniques for summarizing your dataset; but, re-prompting the ChatBot with something like, \"What's the simplest form of summarization of this dataset that I could do and how do I do it in Python?\" or suggesting guidance using the specific summarization methods requested above will helpfully re-orient the ChatBot to your specific interests and needs\n",
    "> \n",
    "> #### Jupyter Notebook Hints\n",
    "> \n",
    "> Jupyter notebook printouts usaully don't show all of the data (when there's too much to show, like if `df.describe()` includes results for many columns), but the printouts just show enough of the data to give an idea of what the results are which is all we're looking for at the moment\n",
    "> \n",
    "> - Consider dividing the code that ChatBot provides you into different jupyter notebook cells so that each cell concludes with a key printed result; the last line of code in a jupyter notebook cell will automatically print out in a formatted manner, so replacing something like `print(df.head())` with `df.head()` as the last line of a cell provides a sensible way to organize your code\n",
    "> - The printout suggestions above are demonstrated in `STA130F24_CourseProject.ipynb` if looking at an example would be helpful to understand what they're getting at...\n",
    "    \n",
    "</details>\n",
    "                \n",
    "#### 4. If the dataset you're using has (a) non-numeric variables and (b) missing values in numeric variables, explain (perhaps using help from a ChatBot if needed) the discrepancies between size of the dataset given by `df.shape` and what is reported by `df.describe()` with respect to (a) the number of columns it analyzes and (b) the values it reports in the \"count\" column<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> If the dataset you're using does not have (a) non-numeric variables and (b) missing values in numeric variables (e.g., the `\"villagers.csv\"` example above has only a single numeric variable `row_n` which has no missing values), instead download and use the [https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv](https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv)\" data to answer this question  \n",
    ">\n",
    "> In (a) above, the \"columns it analyzes\" refers to the columns of the output of `df.describe()` which will only include \"numeric\" columns by default, but you can can see the names of all the columns in a dataset using `df.columns`; and, make sure `df.shape` is refering to the dataset you think it is... if you've loaded a different dataset it might not have been called `df`(!)\n",
    ">\n",
    "> **If you get any errors (for example related to column names), copy and paste them as a response to the ChatBot, and see if it can help you resove them by adding the suggested adjustments to your code and then reruning all your code to see if the changes have fixed the problem (and repeat this process as needed until the problems have been resolved).**\n",
    "    \n",
    "</details>\n",
    "    \n",
    "#### 5. Use your ChatBot session to help understand the difference between the following and then provide your own paraphrasing summarization of that difference\n",
    "\n",
    "- an \"attribute\", such as `df.shape` which does not end with `()`\n",
    "- and a \"method\", such as `df.describe()` which does end with `()` \n",
    "   \n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> The fact that a \"method\" such as `df.describe()` ends with `()` suggests that \"methods\" are essentially something that we would call a \"function\" in programming language terminology; but, without getting too technical or \"in the weeds\", it might also be worth considering that we could also contrast what the difference is between a \"function\" in a programming language versus a \"function\" in mathematics...  \n",
    "    \n",
    "</details><br><br>\n",
    "\n",
    "***Don't forget to ask for summaries of your ChatBot session(s) and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatGPT)!***<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63051fe8",
   "metadata": {},
   "source": [
    "# Pre Lecture Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b347f047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     row_n        id      name  gender    species birthday personality  \\\n",
      "0        2   admiral   Admiral    male       bird     1-27      cranky   \n",
      "1        3   agent-s   Agent S  female   squirrel      7-2       peppy   \n",
      "2        4     agnes     Agnes  female        pig     4-21        uchi   \n",
      "3        6        al        Al    male    gorilla    10-18        lazy   \n",
      "4        7   alfonso   Alfonso    male  alligator      6-9        lazy   \n",
      "..     ...       ...       ...     ...        ...      ...         ...   \n",
      "386    475    winnie    Winnie  female      horse     1-31       peppy   \n",
      "387    477  wolfgang  Wolfgang    male       wolf    11-25      cranky   \n",
      "388    480      yuka      Yuka  female      koala     7-20      snooty   \n",
      "389    481      zell      Zell    male       deer      6-7        smug   \n",
      "390    483    zucker    Zucker    male    octopus      3-8        lazy   \n",
      "\n",
      "                song    phrase            full_id  \\\n",
      "0         Steep Hill   aye aye   villager-admiral   \n",
      "1            DJ K.K.  sidekick   villager-agent-s   \n",
      "2         K.K. House   snuffle     villager-agnes   \n",
      "3         Steep Hill   Ayyeeee        villager-al   \n",
      "4        Forest Life  it'sa me   villager-alfonso   \n",
      "..               ...       ...                ...   \n",
      "386         My Place    hay-OK    villager-winnie   \n",
      "387        K.K. Song   snarrrl  villager-wolfgang   \n",
      "388     Soulful K.K.   tsk tsk      villager-yuka   \n",
      "389         K.K. D&B     pronk      villager-zell   \n",
      "390  Spring Blossoms     bloop    villager-zucker   \n",
      "\n",
      "                                                   url  \n",
      "0    https://villagerdb.com/images/villagers/thumb/...  \n",
      "1    https://villagerdb.com/images/villagers/thumb/...  \n",
      "2    https://villagerdb.com/images/villagers/thumb/...  \n",
      "3    https://villagerdb.com/images/villagers/thumb/...  \n",
      "4    https://villagerdb.com/images/villagers/thumb/...  \n",
      "..                                                 ...  \n",
      "386  https://villagerdb.com/images/villagers/thumb/...  \n",
      "387  https://villagerdb.com/images/villagers/thumb/...  \n",
      "388  https://villagerdb.com/images/villagers/thumb/...  \n",
      "389  https://villagerdb.com/images/villagers/thumb/...  \n",
      "390  https://villagerdb.com/images/villagers/thumb/...  \n",
      "\n",
      "[391 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67d2f7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_n           0\n",
      "id              1\n",
      "name            0\n",
      "gender          0\n",
      "species         0\n",
      "birthday        0\n",
      "personality     0\n",
      "song           11\n",
      "phrase          0\n",
      "full_id         0\n",
      "url             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "missing_values = df.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea57fbf",
   "metadata": {},
   "source": [
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Continue now...?</u></summary>\n",
    "\n",
    "### Prelecture VS Postlecture HW\n",
    "    \n",
    "Feel free to work on the \"Postlecture\" HW below if you're making good progress and want to continue: in this case this is particularly reasonable as questions \"6\" and \"7\" below directly follow up and extend the \"Prelecture\" HW questions\n",
    "\n",
    "*The benefits of continue would are that (a) it might be fun to try to tackle the challenge of working through some problems without additional preparation or guidance; and (b) this is a very valable skill to be comfortable with; and (c) it will let you build experience interacting with ChatBots (and beginning to understand their strengths and limitations in this regard)... it's good to have sense of when using a ChatBot is the best way to figure something out, or if another approach (such as course provided resources or a plain old websearch for the right resourse) would be more effective*\n",
    "    \n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d317815d",
   "metadata": {},
   "source": [
    "Thus, we can see there are 11 missing values in the song column in the data set, and 1 missing value in the id column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171693b",
   "metadata": {},
   "source": [
    "# Pre Lecture Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60adf367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 391 rows and 11 columns.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "rows, columns = df.shape\n",
    "print(f\"The dataset has {rows} rows and {columns} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b4694",
   "metadata": {},
   "source": [
    "This means that the data set has 11 columns that show information about the name, gener, species, birthday, the type of personality, the songs, and the id information of 391 villagers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc060493",
   "metadata": {},
   "source": [
    "# Pre lecture Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c525079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of numerical columns:\n",
      "            row_n\n",
      "count  391.000000\n",
      "mean   239.902813\n",
      "std    140.702672\n",
      "min      2.000000\n",
      "25%    117.500000\n",
      "50%    240.000000\n",
      "75%    363.500000\n",
      "max    483.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(\"Summary of numerical columns:\")\n",
    "print(df.describe())\n",
    "#This code is to show the mean, median, and other important data summarized of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c8dc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "General info on the dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 391 entries, 0 to 390\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   row_n        391 non-null    int64 \n",
      " 1   id           390 non-null    object\n",
      " 2   name         391 non-null    object\n",
      " 3   gender       391 non-null    object\n",
      " 4   species      391 non-null    object\n",
      " 5   birthday     391 non-null    object\n",
      " 6   personality  391 non-null    object\n",
      " 7   song         380 non-null    object\n",
      " 8   phrase       391 non-null    object\n",
      " 9   full_id      391 non-null    object\n",
      " 10  url          391 non-null    object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 33.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(\"\\nGeneral info on the dataset:\")\n",
    "print(df.info())\n",
    "#This code is to show the column names, missing values, and the type of the data eg: if the data is an object, int, str, bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f906fecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts for a specific column:\n",
      "name\n",
      "Admiral     1\n",
      "Agent S     1\n",
      "Bea         1\n",
      "Barold      1\n",
      "Bangle      1\n",
      "Bam         1\n",
      "Baabara     1\n",
      "Axel        1\n",
      "Avery       1\n",
      "Ava         1\n",
      "Aurora      1\n",
      "Audie       1\n",
      "Astrid      1\n",
      "Apple       1\n",
      "Apollo      1\n",
      "Antonio     1\n",
      "Annalise    1\n",
      "Annalisa    1\n",
      "Ankha       1\n",
      "Anicotti    1\n",
      "Angus       1\n",
      "Anchovy     1\n",
      "Anabelle    1\n",
      "Amelia      1\n",
      "Alli        1\n",
      "Alice       1\n",
      "Alfonso     1\n",
      "Al          1\n",
      "Agnes       1\n",
      "Beardo      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nValue counts for a specific column:\")\n",
    "print(df.head(30)['name'].value_counts())\n",
    "#This code shows the number of data with the same name/value on a column. For this example, it shows the number in the column name\n",
    "# We use head to show more values on jupiter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb91512",
   "metadata": {},
   "source": [
    "# Pre lecture Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e8b91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
      "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
      "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
      "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
      "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
      "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
      "\n",
      "     who  adult_male deck  embark_town alive  alone  \n",
      "0    man        True  NaN  Southampton    no  False  \n",
      "1  woman       False    C    Cherbourg   yes  False  \n",
      "2  woman       False  NaN  Southampton   yes   True  \n",
      "3  woman       False    C  Southampton   yes  False  \n",
      "4    man        True  NaN  Southampton    no   True  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "516d33ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(891, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0e30cd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         survived      pclass         age       sibsp       parch        fare\n",
       "count  891.000000  891.000000  714.000000  891.000000  891.000000  891.000000\n",
       "mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208\n",
       "std      0.486592    0.836071   14.526497    1.102743    0.806057   49.693429\n",
       "min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000\n",
       "25%      0.000000    2.000000   20.125000    0.000000    0.000000    7.910400\n",
       "50%      0.000000    3.000000   28.000000    0.000000    0.000000   14.454200\n",
       "75%      1.000000    3.000000   38.000000    1.000000    0.000000   31.000000\n",
       "max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b602251a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>889</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>203</td>\n",
       "      <td>889</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>644</td>\n",
       "      <td>491</td>\n",
       "      <td>537</td>\n",
       "      <td>537</td>\n",
       "      <td>59</td>\n",
       "      <td>644</td>\n",
       "      <td>549</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          survived      pclass   sex         age       sibsp       parch  \\\n",
       "count   891.000000  891.000000   891  714.000000  891.000000  891.000000   \n",
       "unique         NaN         NaN     2         NaN         NaN         NaN   \n",
       "top            NaN         NaN  male         NaN         NaN         NaN   \n",
       "freq           NaN         NaN   577         NaN         NaN         NaN   \n",
       "mean      0.383838    2.308642   NaN   29.699118    0.523008    0.381594   \n",
       "std       0.486592    0.836071   NaN   14.526497    1.102743    0.806057   \n",
       "min       0.000000    1.000000   NaN    0.420000    0.000000    0.000000   \n",
       "25%       0.000000    2.000000   NaN   20.125000    0.000000    0.000000   \n",
       "50%       0.000000    3.000000   NaN   28.000000    0.000000    0.000000   \n",
       "75%       1.000000    3.000000   NaN   38.000000    1.000000    0.000000   \n",
       "max       1.000000    3.000000   NaN   80.000000    8.000000    6.000000   \n",
       "\n",
       "              fare embarked  class  who adult_male deck  embark_town alive  \\\n",
       "count   891.000000      889    891  891        891  203          889   891   \n",
       "unique         NaN        3      3    3          2    7            3     2   \n",
       "top            NaN        S  Third  man       True    C  Southampton    no   \n",
       "freq           NaN      644    491  537        537   59          644   549   \n",
       "mean     32.204208      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
       "std      49.693429      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
       "min       0.000000      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
       "25%       7.910400      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
       "50%      14.454200      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
       "75%      31.000000      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
       "max     512.329200      NaN    NaN  NaN        NaN  NaN          NaN   NaN   \n",
       "\n",
       "       alone  \n",
       "count    891  \n",
       "unique     2  \n",
       "top     True  \n",
       "freq     537  \n",
       "mean     NaN  \n",
       "std      NaN  \n",
       "min      NaN  \n",
       "25%      NaN  \n",
       "50%      NaN  \n",
       "75%      NaN  \n",
       "max      NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3280edba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
       "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
       "       'alive', 'alone'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48ac48cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived         0\n",
       "pclass           0\n",
       "sex              0\n",
       "age            177\n",
       "sibsp            0\n",
       "parch            0\n",
       "fare             0\n",
       "embarked         2\n",
       "class            0\n",
       "who              0\n",
       "adult_male       0\n",
       "deck           688\n",
       "embark_town      2\n",
       "alive            0\n",
       "alone            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9223b",
   "metadata": {},
   "source": [
    "In this data set, we can see how although there are no discrepancies in the number of columns, there is a discrepancie between the value of rows df.shape gives us and the values in df.describe(include='all'). df.shape tells us that there are 891 rows, but in values such as age, embarked, deck, and ebark_town, the number it shows in the count row is less than 891. The explanation for this is most likely because there are parts of the data that are missing and as such, they are not counted in the count value. We check that by using df.isna().sum() to give us the number of missing data in each column, and we see that the numbers are exactly consistent with what we see in df. describe(include='all'). In fact, if we substract 891-177 in age, we get the value in the count row 714. The same applies to all other missing values. As well, there are missing values in the form of NaN from the unique row to the max row in some columns because those columns corresponded to non numerical values and as such, it is impossible to calculate an \"average\" or \"median\" of a string or other types of values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d743e1",
   "metadata": {},
   "source": [
    "# Pre lecture Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fc453",
   "metadata": {},
   "source": [
    "The difference between attributes such as df.shape and methods such as df.head() is that attributes hold data that is already stored. In other words, they do not perform any further action other than giving the data that is already stored in them. As well, they do not accept paranthesis because there is no extra information or \"orders\" you can give them. They just show their value. Methods however, execute an action and accept modifiers (for example, df.head(7) to show only the first 7 values). They do not just acces a value stored, they perform certain action, and as such, a parenthesis is needed to call them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1c7d2e",
   "metadata": {},
   "source": [
    "Transcript of Chat Gpt for the first 5 questions (daily limit was reached)\n",
    "https://chatgpt.com/share/f77e5abc-ee5a-4228-bf4d-c0bdc83f8492"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d258e0",
   "metadata": {},
   "source": [
    "## \"Postlecture\" HW [*submission along with \"Prelecture\" HW is due prior to next TUT*]\n",
    "\n",
    "#### 6. The `df.describe()` method provides the 'count', 'mean', 'std', 'min', '25%', '50%', '75%', and 'max' summary statistics for each variable it analyzes. Give the definitions (perhaps using help from the ChatBot if needed) of each of these summary statistics<br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> The answers here actually make it obvious why these can only be calculated for numeric variables in a dataset, which should help explain the answer to \"4(a)\" and \"4(b)\" above\n",
    ">   \n",
    "> Also notice that when `df.describe()` is used missing values are not explicitly removed, but `df.describe()`  provides answers anyway. Is it clear what `df.describe()` does with the data in each columns it analyzes if there is missing data in the column in question? \n",
    ">\n",
    "> The next questions addresses removing rows or columns from a dataset in order to explicitly remove the presense of any missingness in the dataset (assuming we're not going to fill in any missing data values using any missing data imputation methods, which are beyond the scope of STA130); so, the behavior of `df.describe()` hints that explicitly removing missing may not always be necessary; but, the concern, though, is that not all methods may be able to handle missing data the way `df.describe()` does...\n",
    "    \n",
    "</details>\n",
    "\n",
    "#### 7. Missing data can be considered \"across rows\" or \"down columns\".  Consider how `df.dropna()` or `del df['col']` should be applied to most efficiently use the available non-missing data in your dataset and briefly answer the following questions in your own words\n",
    "\n",
    "1. Provide an example of a \"use case\" in which using `df.dropna()` might be peferred over using `del df['col']`<br><br>\n",
    "    \n",
    "2. Provide an example of \"the opposite use case\" in which using `del df['col']` might be preferred over using `df.dropna()` <br><br>\n",
    "    \n",
    "3. Discuss why applying `del df['col']` before `df.dropna()` when both are used together could be important<br><br>\n",
    "    \n",
    "4. Remove all missing data from one of the datasets you're considering using some combination of `del df['col']` and/or `df.dropna()` and give a justification for your approach, including a \"before and after\" report of the results of your approach for your dataset.<br><br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> Start a new ChatBot session **[but remember to first ask your ChatBot for summaries of your current session and perhaps coding results (so you can supply these in the homework as requested)]**, since your last ChatBot session has likely gotten quite long and has covered a lot of material at this point \n",
    "> - It can sometimes be helpful to reset ChatBot sessions to refocus them on the topics of inquiry without too much backlog history that might unintentionally bias things in certain directions and, of course, you can always re-introduce material from earlier conversations as it's relevant, such as for answering \"D\" based on reintroducing and updating code you made in a previous ChatBot session.  \n",
    "> \n",
    "> #### ChatBot Scope Guidance\n",
    "> \n",
    "> - This question is not interested in the general benefits of imputing missing data, or the general benefits of using `df.dropna()` and/or `del df['col']` to remove missing data, just how to most efficiently remove missing data if a user chooses to do so\n",
    "> \n",
    "> - More sophisticated analyses for \"filling in\" rather than removing missing data (as considered here) are possible (based on making assumptions about missing data and using specific imputation methods or models) but these are \"beyond the scope\" of this homework assignment so this topics can be safely ignored for now\n",
    "> \n",
    "> #### ChatBot Code Troubleshooting\n",
    "> \n",
    "> A key issue to be aware of when asking ChatBots for help with something is that they are not running and checking code for correctess, and they often intertwine written instructions with code instructions; so, BEFORE YOU RUN ANY CODE provided by a ChatBot, you should check the following\n",
    "> \n",
    "> 1. If this code changes an object or data, are you sure you want to run this code?\n",
    "> 2. Can you easily \"undo\" the results of running code (e.g., from a copy `df_saved=df.copy()` or reloading the data) if running the code doesn't do what you want?\n",
    "> 3. Is the state of the data what is expected by the code? Or have the objects been updated and changed so they're no longer what the code expects them to be? \n",
    "> \n",
    "> #### If you get any `Python` errors, copy and paste them into the ChatBot prompt and see if it can help you resove them; but, keep in mind the final point above becasue the ChatBot might not be aware of the state of your objects relative to the code it's producing...\n",
    "\n",
    "</details><br>\n",
    "    \n",
    "#### 8. Give brief explanations in your own words for any requested answers to the questions below\n",
    "\n",
    "> This problem will guide you through exploring how to use a ChatBot to troubleshoot code using the \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\" data set \n",
    "> \n",
    "> To initialially constrain the scope of the reponses from your ChatBot, start a new ChatBot session with the following slight variation on the initial prompting approach from \"2\" above\n",
    "> - \"I am going to do some initial simple summary analyses on the titanic data set I've downloaded (https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv) which has some missing values, and I'd like to get your help understanding the code I'm using and the analysis it's performing\"\n",
    "        \n",
    "1. Use your ChatBot session to understand what `df.groupby(\"col1\")[\"col2\"].describe()` does and then demonstrate and explain this using a different example from the \"titanic\" data set other than what the ChatBot automatically provide for you\n",
    "    \n",
    "> If needed, you can help guide the ChatBot by showing it the code you've used to download the data **AND provide it with the names of the columns** using either a summary of the data with `df.describe()` or just `df.columns` as demonstrated [here](../CHATLOG/COP/00017_copilot_groupby.md)\n",
    "    \n",
    "2. Assuming you've not yet removed missing values in the manner of question \"7\" above, `df.describe()` would have different values in the `count` value for different data columns depending on the missingness present in the original data.  Why do these capture something fundamentally different from the values in the `count` that result from doing something like `df.groupby(\"col1\")[\"col2\"].describe()`?\n",
    "\n",
    "> Questions \"4\" and \"6\" above address how missing values are handled by `df.describe()` (which is reflected in the `count` output of this method); but, `count` in conjunction with `group_by` has another primary function that's more important than addressing missing values (although missing data could still play a role here).\n",
    "\n",
    "3. Intentionally introduce the following errors into your code and report your opinion as to whether it's easier to (a) work in a ChatBot session to fix the errors, or (b) use google to search for and fix errors: first share the errors you get in the ChatBot session and see if you can work with ChatBot to troubleshoot and fix the coding errors, and then see if you think a google search for the error provides the necessary toubleshooting help more quickly than ChatGPT<br><br>\n",
    "    \n",
    "    1. Forget to include `import pandas as pd` in your code \n",
    "       <br> \n",
    "       Use Kernel->Restart from the notebook menu to restart the jupyter notebook session unload imported libraries and start over so you can create this error\n",
    "       <br><br>\n",
    "       When python has an error, it sometimes provides a lot of \"stack trace\" output, but that's not usually very important for troubleshooting. For this problem for example, all you need to share with ChatGPT or search on google is `\"NameError: name 'pd' is not defined\"`<br><br>\n",
    "\n",
    "    2. Mistype \"titanic.csv\" as \"titanics.csv\"\n",
    "       <br> \n",
    "       If ChatBot troubleshooting is based on downloading the file, just replace the whole url with \"titanics.csv\" and try to troubleshoot the subsequent `FileNotFoundError: [Errno 2] No such file or directory: 'titanics.csv'` (assuming the file is indeed not present)\n",
    "       <br><br>\n",
    "       Explore introducing typos into a couple other parts of the url and note the slightly different errors this produces<br><br>\n",
    "      \n",
    "    3. Try to use a dataframe before it's been assigned into the variable\n",
    "       <br> \n",
    "       You can simulate this by just misnaming the variable. For example, if you should write `df.groupby(\"col1\")[\"col2\"].describe()` based on how you loaded the data, then instead write `DF.groupby(\"col1\")[\"col2\"].describe()`\n",
    "       <br><br>\n",
    "       Make sure you've fixed your file name so that's not the error any more<br><br>\n",
    "        \n",
    "    4. Forget one of the parentheses somewhere the code\n",
    "       <br>\n",
    "       For example, if the code should be `pd.read_csv(url)` the change it to `pd.read_csv(url`<br><br>\n",
    "        \n",
    "    5. Mistype one of the names of the chained functions with the code \n",
    "       <br>\n",
    "       For example, try something like `df.group_by(\"col1\")[\"col2\"].describe()` and `df.groupby(\"col1\")[\"col2\"].describle()`<br><br>\n",
    "        \n",
    "    6. Use a column name that's not in your data for the `groupby` and column selection \n",
    "       <br>\n",
    "       For example, try capitalizing the columns for example replacing \"sex\" with \"Sex\" in `titanic_df.groupby(\"sex\")[\"age\"].describe()`, and then instead introducing the same error of \"age\"<br><br>\n",
    "        \n",
    "    7. Forget to put the column name as a string in quotes for the `groupby` and column selection, and see if the ChatBot and google are still as helpful as they were for the previous question\n",
    "       <br>\n",
    "       For example, something like `titanic_df.groupby(sex)[\"age\"].describe()`, and then `titanic_df.groupby(\"sex\")[age].describe()`\n",
    "        \n",
    "#### 9. Have you reviewed the course [wiki-textbook](https://github.com/pointOfive/stat130chat130/wiki) and interacted with a ChatBot (or, if that wasn't sufficient, real people in the course piazza discussion board or TA office hours) to help you understand all the material in the tutorial and lecture that you didn't quite follow when you first saw it?<br>\n",
    "    \n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Further Guidance</u></summary>\n",
    "\n",
    "> Just answering \"Yes\" or \"No\" or \"Somewhat\" or \"Mostly\" or whatever here is fine as this question isn't a part of the rubric; but, the midterm and final exams may ask questions that are based on the tutorial and lecture materials; and, your own skills will be limited by your familiarity with these materials (which will determine your ability to actually do actual things effectively with these skills... like the course project...)\n",
    "    \n",
    "</details>\n",
    "    \n",
    "***Don't forget to ask for summaries of your ChatBot session(s) and paste these into your homework notebook (including link(s) to chat log histories if you're using ChatGPT)!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7876b",
   "metadata": {},
   "source": [
    "# Post Lecture Question 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b7e473",
   "metadata": {},
   "source": [
    "Mean: the average for the data points or \"observations\" in a data set\n",
    "\n",
    "Std=Standard Deviation: Indicates how disperse the data is with respect to its average, in other words, the average distance of ll the points from the mean. + std = + dispersion of data.\n",
    "\n",
    "Count: The number of data points in the data set.\n",
    "\n",
    "min: the smallest value in the data set\n",
    "\n",
    "25% quartile: The value below which 25% of the data points are in. Below that point, 25% of the data is there.\n",
    "\n",
    "50% or the median: The middle value of the data set when it is organized from smallest to largest.\n",
    "\n",
    "75% quartile: The value below which 75% of the data points are in. Below that point, 75% of the data is there.\n",
    "\n",
    "Max: The maximum value in the data set. The largest one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ab7cc",
   "metadata": {},
   "source": [
    "Chat Gpt Transcript: https://chatgpt.com/c/66e377a3-3714-8011-9a96-808bbf40af07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5584e1",
   "metadata": {},
   "source": [
    "# Post Lecture Question 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aebbd4",
   "metadata": {},
   "source": [
    "1.df.dropna() provides an advantage in efficiency over del df[''] because with dropna, you can automatically drop all the columns that have missing values, making it faster to due future analysis of data in the data set that require no NaN data.\n",
    "\n",
    "2.If there is a case in which we want to purposelly select an specific column to delete and dont have in account in a data analyisis, it is much better to use del df[''] as by putting the name of the column you can seletivelly delete any column.\n",
    "\n",
    "3.If we wanted to delete certain column with NaN values that we dont want to influence the outcomes of a future data analysis, if we used df.dropna() before del df[''], df.dropna() would delete all rows that have NaN values, affecting other columns as well that we may want to use. For this reason, using del df[''] ensures that we first delete the column with NaN values so when we use df.dropna(), it doesn't take into account the previous existing NaN values in the column we deleted using del df[''] when it drops the other rows with NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d69fbc2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_n</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>species</th>\n",
       "      <th>birthday</th>\n",
       "      <th>personality</th>\n",
       "      <th>song</th>\n",
       "      <th>phrase</th>\n",
       "      <th>full_id</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>391.000000</td>\n",
       "      <td>390</td>\n",
       "      <td>391</td>\n",
       "      <td>391</td>\n",
       "      <td>391</td>\n",
       "      <td>391</td>\n",
       "      <td>391</td>\n",
       "      <td>380</td>\n",
       "      <td>391</td>\n",
       "      <td>391</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>390</td>\n",
       "      <td>391</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>361</td>\n",
       "      <td>8</td>\n",
       "      <td>92</td>\n",
       "      <td>388</td>\n",
       "      <td>391</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>admiral</td>\n",
       "      <td>Admiral</td>\n",
       "      <td>male</td>\n",
       "      <td>cat</td>\n",
       "      <td>1-27</td>\n",
       "      <td>lazy</td>\n",
       "      <td>K.K. Country</td>\n",
       "      <td>wee one</td>\n",
       "      <td>villager-admiral</td>\n",
       "      <td>https://villagerdb.com/images/villagers/thumb/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>204</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>239.902813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>140.702672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>117.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>240.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>363.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>483.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             row_n       id     name gender species birthday personality  \\\n",
       "count   391.000000      390      391    391     391      391         391   \n",
       "unique         NaN      390      391      2      35      361           8   \n",
       "top            NaN  admiral  Admiral   male     cat     1-27        lazy   \n",
       "freq           NaN        1        1    204      23        2          60   \n",
       "mean    239.902813      NaN      NaN    NaN     NaN      NaN         NaN   \n",
       "std     140.702672      NaN      NaN    NaN     NaN      NaN         NaN   \n",
       "min       2.000000      NaN      NaN    NaN     NaN      NaN         NaN   \n",
       "25%     117.500000      NaN      NaN    NaN     NaN      NaN         NaN   \n",
       "50%     240.000000      NaN      NaN    NaN     NaN      NaN         NaN   \n",
       "75%     363.500000      NaN      NaN    NaN     NaN      NaN         NaN   \n",
       "max     483.000000      NaN      NaN    NaN     NaN      NaN         NaN   \n",
       "\n",
       "                song   phrase           full_id  \\\n",
       "count            380      391               391   \n",
       "unique            92      388               391   \n",
       "top     K.K. Country  wee one  villager-admiral   \n",
       "freq              10        2                 1   \n",
       "mean             NaN      NaN               NaN   \n",
       "std              NaN      NaN               NaN   \n",
       "min              NaN      NaN               NaN   \n",
       "25%              NaN      NaN               NaN   \n",
       "50%              NaN      NaN               NaN   \n",
       "75%              NaN      NaN               NaN   \n",
       "max              NaN      NaN               NaN   \n",
       "\n",
       "                                                      url  \n",
       "count                                                 391  \n",
       "unique                                                391  \n",
       "top     https://villagerdb.com/images/villagers/thumb/...  \n",
       "freq                                                    1  \n",
       "mean                                                  NaN  \n",
       "std                                                   NaN  \n",
       "min                                                   NaN  \n",
       "25%                                                   NaN  \n",
       "50%                                                   NaN  \n",
       "75%                                                   NaN  \n",
       "max                                                   NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.describe(include='all')\n",
    "#Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe5aa0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>391.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>239.902813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>140.702672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>117.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>240.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>363.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>483.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            row_n\n",
       "count  391.000000\n",
       "mean   239.902813\n",
       "std    140.702672\n",
       "min      2.000000\n",
       "25%    117.500000\n",
       "50%    240.000000\n",
       "75%    363.500000\n",
       "max    483.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "del df['id']\n",
    "del df['name']\n",
    "del df['gender']\n",
    "del df['species']\n",
    "del df['birthday']\n",
    "del df['personality']\n",
    "del df['song']\n",
    "del df['phrase']\n",
    "del df['full_id']\n",
    "del df['url']\n",
    "df.dropna()\n",
    "df.describe(include='all')\n",
    "#After"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e6bf19",
   "metadata": {},
   "source": [
    "I used this approach because I wanted to make a data anaylisis of only numerical values. Because of this, I had to delete NaN values. I could use dropna first, but that would have deleted all the values in mean, std... max rows as in other columns there where NaN values. Because of this, I first needed to delete the columns with this NaN values, which I used del df[''] several times. Finally, to get rid of the remaining rows with NaN, I used df.dropna() to automatize the process, leaving me with data to make my numerical analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5446480",
   "metadata": {},
   "source": [
    "Chat bot transcript: https://chatgpt.com/share/9189f285-ddc1-448b-8c42-665a5c2cde4e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66240ee2",
   "metadata": {},
   "source": [
    "# Post lecture Question 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a0847",
   "metadata": {},
   "source": [
    "Subquestion 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d506e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sepal_length  sepal_width  petal_length  petal_width        class\n",
      "0            5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1            4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2            4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3            4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4            5.0          3.6           1.4          0.2  Iris-setosa\n",
      "5            5.4          3.9           1.7          0.4  Iris-setosa\n",
      "6            4.6          3.4           1.4          0.3  Iris-setosa\n",
      "7            5.0          3.4           1.5          0.2  Iris-setosa\n",
      "8            4.4          2.9           1.4          0.2  Iris-setosa\n",
      "9            4.9          3.1           1.5          0.1  Iris-setosa\n",
      "10           5.4          3.7           1.5          0.2  Iris-setosa\n",
      "11           4.8          3.4           1.6          0.2  Iris-setosa\n",
      "12           4.8          3.0           1.4          0.1  Iris-setosa\n",
      "13           4.3          3.0           1.1          0.1  Iris-setosa\n",
      "14           5.8          4.0           1.2          0.2  Iris-setosa\n",
      "15           5.7          4.4           1.5          0.4  Iris-setosa\n",
      "16           5.4          3.9           1.3          0.4  Iris-setosa\n",
      "17           5.1          3.5           1.4          0.3  Iris-setosa\n",
      "18           5.7          3.8           1.7          0.3  Iris-setosa\n",
      "19           5.1          3.8           1.5          0.3  Iris-setosa\n",
      "20           5.4          3.4           1.7          0.2  Iris-setosa\n",
      "21           5.1          3.7           1.5          0.4  Iris-setosa\n",
      "22           4.6          3.6           1.0          0.2  Iris-setosa\n",
      "23           5.1          3.3           1.7          0.5  Iris-setosa\n",
      "24           4.8          3.4           1.9          0.2  Iris-setosa\n",
      "25           5.0          3.0           1.6          0.2  Iris-setosa\n",
      "26           5.0          3.4           1.6          0.4  Iris-setosa\n",
      "27           5.2          3.5           1.5          0.2  Iris-setosa\n",
      "28           5.2          3.4           1.4          0.2  Iris-setosa\n",
      "29           4.7          3.2           1.6          0.2  Iris-setosa\n",
      "30           4.8          3.1           1.6          0.2  Iris-setosa\n",
      "31           5.4          3.4           1.5          0.4  Iris-setosa\n",
      "32           5.2          4.1           1.5          0.1  Iris-setosa\n",
      "33           5.5          4.2           1.4          0.2  Iris-setosa\n",
      "34           4.9          3.1           1.5          0.1  Iris-setosa\n",
      "35           5.0          3.2           1.2          0.2  Iris-setosa\n",
      "36           5.5          3.5           1.3          0.2  Iris-setosa\n",
      "37           4.9          3.1           1.5          0.1  Iris-setosa\n",
      "38           4.4          3.0           1.3          0.2  Iris-setosa\n",
      "39           5.1          3.4           1.5          0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "df = pd.read_csv(url, header=None, names=columns)\n",
    "print(df.head(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcacbea4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal_length</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4.3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.033333</td>\n",
       "      <td>0.152753</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.950</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.100</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.300</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.300</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.325000</td>\n",
       "      <td>0.221736</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.175</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.450</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.204939</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.400</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.9</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.866667</td>\n",
       "      <td>0.326599</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.625</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.100</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3.120000</td>\n",
       "      <td>0.543241</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.050</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.475</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.477778</td>\n",
       "      <td>0.411636</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.400</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.800</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.425000</td>\n",
       "      <td>0.573730</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.225</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.650</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.700</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.700</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>0.350714</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.400</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.850</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.5</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.842857</td>\n",
       "      <td>0.723089</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.400</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.050</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.816667</td>\n",
       "      <td>0.194079</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.725</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.975</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.656832</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.85</td>\n",
       "      <td>3.200</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.885714</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.750</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.9</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>0.115470</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.100</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.733333</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.325</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.975</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>0.151658</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.800</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.975</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.825000</td>\n",
       "      <td>0.492443</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.650</td>\n",
       "      <td>2.85</td>\n",
       "      <td>3.025</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.855556</td>\n",
       "      <td>0.400347</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.300</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.957143</td>\n",
       "      <td>0.207020</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.800</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.150</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>0.070711</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.925</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.975</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>0.250713</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.150</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.900</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.100</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.100</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.125</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.266667</td>\n",
       "      <td>0.305505</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.100</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.400</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.800</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.800</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.7</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>0.525991</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.800</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3.800</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count      mean       std  min    25%   50%    75%  max\n",
       "sepal_length                                                         \n",
       "4.3             1.0  3.000000       NaN  3.0  3.000  3.00  3.000  3.0\n",
       "4.4             3.0  3.033333  0.152753  2.9  2.950  3.00  3.100  3.2\n",
       "4.5             1.0  2.300000       NaN  2.3  2.300  2.30  2.300  2.3\n",
       "4.6             4.0  3.325000  0.221736  3.1  3.175  3.30  3.450  3.6\n",
       "4.7             2.0  3.200000  0.000000  3.2  3.200  3.20  3.200  3.2\n",
       "4.8             5.0  3.180000  0.204939  3.0  3.000  3.10  3.400  3.4\n",
       "4.9             6.0  2.866667  0.326599  2.4  2.625  3.05  3.100  3.1\n",
       "5.0            10.0  3.120000  0.543241  2.0  3.050  3.35  3.475  3.6\n",
       "5.1             9.0  3.477778  0.411636  2.5  3.400  3.50  3.800  3.8\n",
       "5.2             4.0  3.425000  0.573730  2.7  3.225  3.45  3.650  4.1\n",
       "5.3             1.0  3.700000       NaN  3.7  3.700  3.70  3.700  3.7\n",
       "5.4             6.0  3.550000  0.350714  3.0  3.400  3.55  3.850  3.9\n",
       "5.5             7.0  2.842857  0.723089  2.3  2.400  2.50  3.050  4.2\n",
       "5.6             6.0  2.816667  0.194079  2.5  2.725  2.85  2.975  3.0\n",
       "5.7             8.0  3.100000  0.656832  2.5  2.750  2.85  3.200  4.4\n",
       "5.8             7.0  2.885714  0.494734  2.6  2.700  2.70  2.750  4.0\n",
       "5.9             3.0  3.066667  0.115470  3.0  3.000  3.00  3.100  3.2\n",
       "6.0             6.0  2.733333  0.471876  2.2  2.325  2.80  2.975  3.4\n",
       "6.1             6.0  2.850000  0.151658  2.6  2.800  2.85  2.975  3.0\n",
       "6.2             4.0  2.825000  0.492443  2.2  2.650  2.85  3.025  3.4\n",
       "6.3             9.0  2.855556  0.400347  2.3  2.500  2.80  3.300  3.4\n",
       "6.4             7.0  2.957143  0.207020  2.7  2.800  2.90  3.150  3.2\n",
       "6.5             5.0  3.000000  0.141421  2.8  3.000  3.00  3.000  3.2\n",
       "6.6             2.0  2.950000  0.070711  2.9  2.925  2.95  2.975  3.0\n",
       "6.7             8.0  3.050000  0.250713  2.5  3.000  3.10  3.150  3.3\n",
       "6.8             3.0  3.000000  0.200000  2.8  2.900  3.00  3.100  3.2\n",
       "6.9             4.0  3.125000  0.050000  3.1  3.100  3.10  3.125  3.2\n",
       "7.0             1.0  3.200000       NaN  3.2  3.200  3.20  3.200  3.2\n",
       "7.1             1.0  3.000000       NaN  3.0  3.000  3.00  3.000  3.0\n",
       "7.2             3.0  3.266667  0.305505  3.0  3.100  3.20  3.400  3.6\n",
       "7.3             1.0  2.900000       NaN  2.9  2.900  2.90  2.900  2.9\n",
       "7.4             1.0  2.800000       NaN  2.8  2.800  2.80  2.800  2.8\n",
       "7.6             1.0  3.000000       NaN  3.0  3.000  3.00  3.000  3.0\n",
       "7.7             4.0  3.050000  0.525991  2.6  2.750  2.90  3.200  3.8\n",
       "7.9             1.0  3.800000       NaN  3.8  3.800  3.80  3.800  3.8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"sepal_length\")[\"sepal_width\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aadc778",
   "metadata": {},
   "source": [
    "Here, the method gives the mean, median, std, and other numerical analysis, after dividing the values in subgroups. These subrgoups, which contain data of sepal_width, are determined by values in sepal_length. The way it works is that the code checks which values in sepal_length and assigns the corresponding value in the same row in the column sepal_width. If a value in sepal_length repeats, then the corresponding value in the sepal_width column gets added to the subgroup with the other values in the sepal_length repeated values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef27caa",
   "metadata": {},
   "source": [
    "Subquestion 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b58e254",
   "metadata": {},
   "source": [
    "While df.describe() uses the count row to give us the total number of non-null values in the data set, df.groupby(\"col1\")[\"col2\"].describe() gives the count of non null values for each subgroup. In other words, df.groupby(\"col1\")[\"col2\"].describe() assesses the non null values in each subgroup created by the allocation values of 'col1' and 'col2, giving the count for each subrgroup, and df.describe() gives it for the whole data base with no subgroups. Because of this, doing df.describe() in a data set such as the iris data set would give us diferent values than if we did the df.groupby(\"col1\")[\"col2\"].describe(). Using df.groupby(\"col1\")[\"col2\"].describe() provides an advantage over df.describe() as it gives us data for each corresponding subrgroup, making it easier for us statisticians to understand the missingness of data according to each subgroup in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27e969",
   "metadata": {},
   "source": [
    "Chat Gtp transcript: https://chatgpt.com/share/d787751b-5868-4bc4-b24a-3f8277087843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06d72d0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length  sepal_width  petal_length  petal_width\n",
       "count    150.000000   150.000000    150.000000   150.000000\n",
       "mean       5.843333     3.054000      3.758667     1.198667\n",
       "std        0.828066     0.433594      1.764420     0.763161\n",
       "min        4.300000     2.000000      1.000000     0.100000\n",
       "25%        5.100000     2.800000      1.600000     0.300000\n",
       "50%        5.800000     3.000000      4.350000     1.300000\n",
       "75%        6.400000     3.300000      5.100000     1.800000\n",
       "max        7.900000     4.400000      6.900000     2.500000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "df = pd.read_csv(url, header=None, names=columns)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2154ee3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sepal_length</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4.3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.033333</td>\n",
       "      <td>0.152753</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.950</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.100</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.300</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.300</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.325000</td>\n",
       "      <td>0.221736</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.175</td>\n",
       "      <td>3.30</td>\n",
       "      <td>3.450</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.7</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.8</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.204939</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.400</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.9</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.866667</td>\n",
       "      <td>0.326599</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2.625</td>\n",
       "      <td>3.05</td>\n",
       "      <td>3.100</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>10.0</td>\n",
       "      <td>3.120000</td>\n",
       "      <td>0.543241</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.050</td>\n",
       "      <td>3.35</td>\n",
       "      <td>3.475</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>3.477778</td>\n",
       "      <td>0.411636</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.400</td>\n",
       "      <td>3.50</td>\n",
       "      <td>3.800</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.425000</td>\n",
       "      <td>0.573730</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.225</td>\n",
       "      <td>3.45</td>\n",
       "      <td>3.650</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.7</td>\n",
       "      <td>3.700</td>\n",
       "      <td>3.70</td>\n",
       "      <td>3.700</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.4</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.550000</td>\n",
       "      <td>0.350714</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.400</td>\n",
       "      <td>3.55</td>\n",
       "      <td>3.850</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.5</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.842857</td>\n",
       "      <td>0.723089</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.400</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3.050</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.6</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.816667</td>\n",
       "      <td>0.194079</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.725</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.975</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>0.656832</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.85</td>\n",
       "      <td>3.200</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.885714</td>\n",
       "      <td>0.494734</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.700</td>\n",
       "      <td>2.70</td>\n",
       "      <td>2.750</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.9</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>0.115470</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.100</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.733333</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.325</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.975</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>0.151658</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.800</td>\n",
       "      <td>2.85</td>\n",
       "      <td>2.975</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.825000</td>\n",
       "      <td>0.492443</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.650</td>\n",
       "      <td>2.85</td>\n",
       "      <td>3.025</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.3</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.855556</td>\n",
       "      <td>0.400347</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.500</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.300</td>\n",
       "      <td>3.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2.957143</td>\n",
       "      <td>0.207020</td>\n",
       "      <td>2.7</td>\n",
       "      <td>2.800</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.150</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.141421</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.6</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>0.070711</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.925</td>\n",
       "      <td>2.95</td>\n",
       "      <td>2.975</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>0.250713</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.150</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.900</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.100</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>3.1</td>\n",
       "      <td>3.100</td>\n",
       "      <td>3.10</td>\n",
       "      <td>3.125</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.2</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.266667</td>\n",
       "      <td>0.305505</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.100</td>\n",
       "      <td>3.20</td>\n",
       "      <td>3.400</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.90</td>\n",
       "      <td>2.900</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2.800</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.800</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.000</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.7</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>0.525991</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.750</td>\n",
       "      <td>2.90</td>\n",
       "      <td>3.200</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.800</td>\n",
       "      <td>3.80</td>\n",
       "      <td>3.800</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count      mean       std  min    25%   50%    75%  max\n",
       "sepal_length                                                         \n",
       "4.3             1.0  3.000000       NaN  3.0  3.000  3.00  3.000  3.0\n",
       "4.4             3.0  3.033333  0.152753  2.9  2.950  3.00  3.100  3.2\n",
       "4.5             1.0  2.300000       NaN  2.3  2.300  2.30  2.300  2.3\n",
       "4.6             4.0  3.325000  0.221736  3.1  3.175  3.30  3.450  3.6\n",
       "4.7             2.0  3.200000  0.000000  3.2  3.200  3.20  3.200  3.2\n",
       "4.8             5.0  3.180000  0.204939  3.0  3.000  3.10  3.400  3.4\n",
       "4.9             6.0  2.866667  0.326599  2.4  2.625  3.05  3.100  3.1\n",
       "5.0            10.0  3.120000  0.543241  2.0  3.050  3.35  3.475  3.6\n",
       "5.1             9.0  3.477778  0.411636  2.5  3.400  3.50  3.800  3.8\n",
       "5.2             4.0  3.425000  0.573730  2.7  3.225  3.45  3.650  4.1\n",
       "5.3             1.0  3.700000       NaN  3.7  3.700  3.70  3.700  3.7\n",
       "5.4             6.0  3.550000  0.350714  3.0  3.400  3.55  3.850  3.9\n",
       "5.5             7.0  2.842857  0.723089  2.3  2.400  2.50  3.050  4.2\n",
       "5.6             6.0  2.816667  0.194079  2.5  2.725  2.85  2.975  3.0\n",
       "5.7             8.0  3.100000  0.656832  2.5  2.750  2.85  3.200  4.4\n",
       "5.8             7.0  2.885714  0.494734  2.6  2.700  2.70  2.750  4.0\n",
       "5.9             3.0  3.066667  0.115470  3.0  3.000  3.00  3.100  3.2\n",
       "6.0             6.0  2.733333  0.471876  2.2  2.325  2.80  2.975  3.4\n",
       "6.1             6.0  2.850000  0.151658  2.6  2.800  2.85  2.975  3.0\n",
       "6.2             4.0  2.825000  0.492443  2.2  2.650  2.85  3.025  3.4\n",
       "6.3             9.0  2.855556  0.400347  2.3  2.500  2.80  3.300  3.4\n",
       "6.4             7.0  2.957143  0.207020  2.7  2.800  2.90  3.150  3.2\n",
       "6.5             5.0  3.000000  0.141421  2.8  3.000  3.00  3.000  3.2\n",
       "6.6             2.0  2.950000  0.070711  2.9  2.925  2.95  2.975  3.0\n",
       "6.7             8.0  3.050000  0.250713  2.5  3.000  3.10  3.150  3.3\n",
       "6.8             3.0  3.000000  0.200000  2.8  2.900  3.00  3.100  3.2\n",
       "6.9             4.0  3.125000  0.050000  3.1  3.100  3.10  3.125  3.2\n",
       "7.0             1.0  3.200000       NaN  3.2  3.200  3.20  3.200  3.2\n",
       "7.1             1.0  3.000000       NaN  3.0  3.000  3.00  3.000  3.0\n",
       "7.2             3.0  3.266667  0.305505  3.0  3.100  3.20  3.400  3.6\n",
       "7.3             1.0  2.900000       NaN  2.9  2.900  2.90  2.900  2.9\n",
       "7.4             1.0  2.800000       NaN  2.8  2.800  2.80  2.800  2.8\n",
       "7.6             1.0  3.000000       NaN  3.0  3.000  3.00  3.000  3.0\n",
       "7.7             4.0  3.050000  0.525991  2.6  2.750  2.90  3.200  3.8\n",
       "7.9             1.0  3.800000       NaN  3.8  3.800  3.80  3.800  3.8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"sepal_length\")[\"sepal_width\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea15ed7",
   "metadata": {},
   "source": [
    "Subquestion 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b274339",
   "metadata": {},
   "source": [
    "A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f5d1ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Forgeting to include import pandas\u001b[39;00m\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#Forgeting to include import pandas\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff44a47",
   "metadata": {},
   "source": [
    "In this case, although both platforms provided answers fairly quickly and accesible, google proved to be better in terms of detail and options. Chat Gpt was still accesible and fast however, but for in quickly, in depth analysis, google was better for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a4b0e",
   "metadata": {},
   "source": [
    "B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9501e2a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
      "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
      "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
      "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
      "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
      "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
      "\n",
      "     who  adult_male deck  embark_town alive  alone  \n",
      "0    man        True  NaN  Southampton    no  False  \n",
      "1  woman       False    C    Cherbourg   yes  False  \n",
      "2  woman       False  NaN  Southampton   yes   True  \n",
      "3  woman       False    C  Southampton   yes  False  \n",
      "4    man        True  NaN  Southampton    no   True  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccdef6e",
   "metadata": {},
   "source": [
    "Here, Chat Gpt proved a better option, as it quickly told me that the most likely reason was that the url was incorrect. It even corrected the link for me and explained why it was the correct version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c03b8f",
   "metadata": {},
   "source": [
    "C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b22464bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDF\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DF' is not defined"
     ]
    }
   ],
   "source": [
    "DF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062c3c2",
   "metadata": {},
   "source": [
    "Once again, Chat Gpt proved to be the best alternative in comparison to ggogle, as it stated teh diference between DF and df, showing me they are diferent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f739c26",
   "metadata": {},
   "source": [
    "D:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93d1433b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2352208572.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    df.head(5\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "df.head(5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ca3ba",
   "metadata": {},
   "source": [
    "Although both gave answers, Chat Gpt's answer was way more centralized and stated in a easy to understand way the solution. Google's answer was the result of answers from forums and as such, there where a variety of opinions and explanations, making it harder to find the adequate solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a818686",
   "metadata": {},
   "source": [
    "E:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4228c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'group_by'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_52/42089572.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_by\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sex\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"survived\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'group_by'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.group_by(\"sex\")[\"survived\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f59cf7",
   "metadata": {},
   "source": [
    "Chat Gpt provided me with the correct answer on the first try, showing that it had a better solution than putting the error on google."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47013d90",
   "metadata": {},
   "source": [
    "F:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "881db6fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'columnnull'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumnnull\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSurvived\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'columnnull'"
     ]
    }
   ],
   "source": [
    "df.groupby(\"columnnull\")[\"Survived\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90198297",
   "metadata": {},
   "source": [
    "The ChatBot made the troubleshooting easier, providing me with the answer almost inmediatelly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b6c9d",
   "metadata": {},
   "source": [
    "G:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc5212ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'survived' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[43msurvived\u001b[49m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membarked\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'survived' is not defined"
     ]
    }
   ],
   "source": [
    "df.groupby(survived)[\"embarked\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f01a081",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Survived'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSurvived\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membarked\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Survived'"
     ]
    }
   ],
   "source": [
    "df.groupby(\"Survived\")[\"embarked\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d5eba8",
   "metadata": {},
   "source": [
    "In this case, both google and Chat Gpt made mistakes when providing me with the right answer. However, Chat Gpt's mistake was just a small mispeliing due to putting a capital S in survived, while google gave a huge amount of info that didn't answered the question. With another prompt, the Chat bot was able to correctly answer the question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36466e3",
   "metadata": {},
   "source": [
    "Chat Gpt Transcript: https://chatgpt.com/share/66e37738-2284-8011-9297-e764381b4b85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49df24e",
   "metadata": {},
   "source": [
    "# Post Lecture Question 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547bcdad",
   "metadata": {},
   "source": [
    "Yes, I did interact with Chat Bots and read part of the textbook. I did not need to use Piazza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce11ef1",
   "metadata": {},
   "source": [
    "# Recommended Additional Useful Activities [Optional]\n",
    "\n",
    "The \"Ethical Profesionalism Considerations\" and \"Current Course Project Capability Level\" sections below **are not a part of the required homework assignment**; rather, they are regular weekly guides covering (a) relevant considerations regarding professional and ethical conduct, and (b) the analysis steps for the STA130 course project that are feasible at the current stage of the course<br><br>\n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Ethical Professionalism Considerations</u></summary>\n",
    "\n",
    "## Ethical Professionalism Considerations\n",
    "\n",
    "> If the observed data is \"no events occured\" does this mean the data is \"missing\" and [should be ignored](https://priceonomics.com/the-space-shuttle-challenger-explosion-and-the-o)?\n",
    "> \n",
    "> - NASA: \\<determines temperature doesn't affects \"o-ring\" by subseting data to just \"o-ring\" incidents\\>\n",
    "> - Also NASA: \\<launches the shuttle on a cold day\\>\n",
    "\n",
    "|No apparent \"o-ring\" failure and temperature relationship|Apparent between \"o-ring\" failure and temperature relationship|\n",
    "|:-|:-|\n",
    "if you just look at \"o-ring\" failure event data|if you instead look at ALL the data as you should|\n",
    "|![](https://etzq49yfnmd.exactdn.com/wp-content/uploads/2022/03/image06-14.png)|![](https://etzq49yfnmd.exactdn.com/wp-content/uploads/2022/03/image02-33.png)|\n",
    "|![](https://upload.wikimedia.org/wikipedia/commons/8/8b/Shuttle_Challenger_explosion.gif?20190203170223)|![](https://i.makeagif.com/media/10-04-2014/nT57xW.gif)|\n",
    "\n",
    "<br>\n",
    "    \n",
    "</details>    \n",
    "\n",
    "<details class=\"details-example\"><summary style=\"color:blue\"><u>Current Course Project Capability Level</u></summary>\n",
    "\n",
    "## Current Course Project Capability Level\n",
    "\n",
    "> The data we'll use for the STA130 course project is based on the [Canadian Social Connection Survey](https://casch.org/cscs). Please see the [data use agreement](https://static1.squarespace.com/static/60283c2e174c122f8ebe0f39/t/6239c284d610f76fed5a2e69/1647952517436/Data+Use+Agreement+for+the+Canadian+Social+Connection+Survey.pdf) regarding the appropriate and ethical professional use of this data (available at the bottom of the [CSCS](https://casch.org/cscs) webpage).\n",
    "> \n",
    "> 1. Have a very quick look at the list of available variables available using the [link](https://drive.google.com/file/d/1ISVymGn-WR1lcRs4psIym2N3or5onNBi/view) (again at the bottom of the [CSCS](https://casch.org/cscs) webpage); then, \n",
    "> 2. examine the code in the first thirteen code cells of [STA130F24_CourseProject.ipynb](https://github.com/pointOfive/stat130chat130/blob/main/CP/STA130F24_CourseProject.ipynb) to get an initital understanding of how we might subset to different studies included in the [data](https://drive.google.com/file/d/1mbUQlMTrNYA7Ly5eImVRBn16Ehy9Lggo/view) (again accessible at the bottom of the [CSCS](https://casch.org/cscs) webpage); then,     \n",
    "> 3. review the fourteenth and fifteenth cells (with the comments \"Here's a high level summary of the data\" and \"And here are some explanations about the columns in the data\") a little more closely to get a better sense of which columns seem to be the most interesting and whether or not they seem to have a lot of missing data\n",
    "    \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca7d22",
   "metadata": {},
   "source": [
    "### Afterward\n",
    "\n",
    "Here are few ideas of some other kinds of interactions you might consider exploring with a ChatBot...\n",
    "\n",
    "> While these are likely to be extremely practically valuable, they are not a part of the homework assignment, so do not include anything related to these in your homework submission\n",
    "\n",
    "- With respect to improving ones ability in statistics, coding, communication, and other key data science skills\n",
    "    - what is the ChatBots perception its own capabilities and uses as an AI-driven assistance tool \n",
    "    - and does ChatBots assessment of itself influence or agree with your own evalution of the ChatBot? \n",
    "\n",
    "- ChatBots can introduce and explain the \"World War 2 planes\" problem and the \"Monte Hall\" problem... \n",
    "    - how well does do they seem to do and introducing and explaining other \"unintuitive surprising statistics paradoxes\"?\n",
    "\n",
    "- If you consider the process of writing about why you chose to take this course, and the skills you were hoping to build through this course with respect to your current ideas about what possible careers \n",
    "    - and how do you think the exercise would be different if you framed it as a dialogue with a ChatBot\n",
    "    - and do you think the difference could be positive and productive, or potentially biasing and distracting?\n",
    "    \n",
    "- ChatBots sometimes immediately responds in simple helpful ways, but other times it gives a lot of extraneous information that can be overwheling... are you able to prompt and interact with ChatBots in manner that keeps its reponses helpful and focused on what you're interested in? \n",
    "\n",
    "- ChatBots tends to respond in a fairly empathetic and supportive tone...\n",
    "    - do you find it helpful to discuss concerns you might have about succeeding in the course (or entering university more generally) with a ChatBot?\n",
    "    \n",
    "- For what purposes and in what contexts do you think a ChatBot could provide suggestions or feedback about your experiences that might be useful? \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
